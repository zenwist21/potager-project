# -*- coding: utf-8 -*-
"""image_classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k0Y95R-RDO5c8ZcaHVwCjVfg-Sho4N1y
"""

# from google.colab import files

# uploaded = files.upload()

# for fn in uploaded.keys():
#   print('User uploaded file "{name}" with length {length} bytes'.format(
#       name=fn, length=len(uploaded[fn])))

# Then move kaggle.json into the folder where the API expects to find it.
! mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d abdallahalidev/plantvillage-dataset
! kaggle datasets download -d kareem3egm/cucumber-plant-diseases-dataset

! wget -O ibean-training-set.zip https://storage.googleapis.com/ibeans/train.zip
! wget -O ibean-validation-set.zip https://storage.googleapis.com/ibeans/validation.zip
! wget -O ibean-testing-set.zip https://storage.googleapis.com/ibeans/test.zip

import zipfile
import os, shutil

def extract(source, destination):
    zip = zipfile.ZipFile(source, "r")
    zip.extractall(destination)
    zip.close()
    print("[SUCCESS] {} berhasil diekstrak!".format(os.path.basename(source)))

# PlantVillage and Cucumber dataset

plantvillage_src = "/content/plantvillage-dataset.zip"
cucumber_src = "/content/cucumber-plant-diseases-dataset.zip"

destination = "/content/dataset"

extract(plantvillage_src, destination)
extract(cucumber_src, destination)

# iBean dataset

ibean_train_src = "/content/ibean-training-set.zip"
ibean_val_src = "/content/ibean-validation-set.zip"
ibean_test_src = "/content/ibean-testing-set.zip"

destination = "/content/dataset/ibean-dataset"

extract(ibean_train_src, destination)
extract(ibean_val_src, destination)
extract(ibean_test_src, destination)

def take_and_combine(path, max):
    target = "/content/dataset_combined"
    try:
        for root, dirs, files in os.walk(path, topdown=True):
            if root == path or len(files) == 0:
                continue
            num = 0
            source_path = os.path.join(path, os.path.basename(root))
            target_path = os.path.join(target, os.path.basename(root))
            os.makedirs(target_path)
            for filename in files:
                if num == max:
                    break
                file_source = os.path.join(root, filename)
                file_target = os.path.join(target_path, filename)
                shutil.copy(file_source, file_target)
                num += 1
            print("[SUCCESS] {} berhasil disalin!".format(os.path.basename(target_path)))
    except FileExistsError as error:
        print(error)

def list_dataset(path):
    total = 0
    labels = []
    count = []
    for root, dirs, files in os.walk(path):
        if root == path:
            n_class = len(dirs)
            continue
        basename = os.path.basename(root)
        # print("{} data di kelas {}".format(len(files), basename))
        labels.append(os.path.basename(root))
        count.append(len(files))
        total += len(files)
    print("\nJumlah kelas = {}".format(n_class))
    print("Total data = {}".format(total))
    return labels, count

max = 250

take_and_combine("/content/dataset/plantvillage dataset/color", max)                # PlantVillage
take_and_combine("/content/dataset/Cucumber plant diseases dataset/training", max)  # Cucumber
take_and_combine("/content/dataset/ibean-dataset/train", max)                       # iBean

# Hapus directory (untuk testing)
# shutil.rmtree("/content/dataset_combined")
shutil.rmtree("/content/dataset_combined/Apple___Apple_scab")
shutil.rmtree("/content/dataset_combined/Apple___healthy")
shutil.rmtree("/content/dataset_combined/Apple___Black_rot")
shutil.rmtree("/content/dataset_combined/Apple___Cedar_apple_rust")
shutil.rmtree("/content/dataset_combined/Peach___Bacterial_spot")
shutil.rmtree("/content/dataset_combined/Peach___healthy")
shutil.rmtree("/content/dataset_combined/Cherry_(including_sour)___Powdery_mildew")
shutil.rmtree("/content/dataset_combined/Cherry_(including_sour)___healthy")
shutil.rmtree("/content/dataset_combined/Orange___Haunglongbing_(Citrus_greening)")

try:
    os.rename("/content/dataset_combined/Ill_cucumber", "/content/dataset_combined/Cucumber__Ill")
    os.rename("/content/dataset_combined/good_Cucumber", "/content/dataset_combined/Cucumber__healthy")
    os.rename("/content/dataset_combined/healthy", "/content/dataset_combined/Bean__healthy")
    os.rename("/content/dataset_combined/bean_rust", "/content/dataset_combined/Bean__Bean_rust")
    os.rename("/content/dataset_combined/angular_leaf_spot", "/content/dataset_combined/Bean__Angular_leaf_spot")
except:
    print("Folder tidak ditemukan!")

labels, count = list_dataset("/content/dataset_combined")

import matplotlib.pyplot as plt
import numpy as np

x = np.arange(len(labels))
y = count

plt.figure(figsize=(25,10))
plt.bar(x, y, tick_label=labels)
plt.xlabel('Jenis Tanaman dan Penyakit')
plt.ylabel('Banyak Data')
plt.xticks(rotation=45, ha='right')
plt.show()

print(labels)

index = labels.index('Orange___Haunglongbing_(Citrus_greening)')
num_data = count[index+1]
print(num_data)

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

i = 0
nrows = 43
ncols = 4

fig = plt.gcf()
fig.set_size_inches(ncols*4, nrows*4)

for root, dirs, files in os.walk("/content/dataset_combined"):
    if len(dirs) != 0:
        continue
    for filename in files[:4]:
        i += 1
        filepath = os.path.join(root, filename)
        img = mpimg.imread(filepath)
        label = os.path.basename(root).split("__")[0]
        shapes = img.shape
        plt.title("{} {}".format(label, shapes))
        plt.subplot(nrows, ncols, i)
        plt.imshow(img)
plt.show()

! pip install split-folders

import splitfolders

main_dir = "/content/dataset_split"
train_dir = os.path.join(main_dir, "train")
val_dir = os.path.join(main_dir, "val")

splitfolders.ratio("/content/dataset_combined", output=main_dir, seed=42, ratio=(.8, .2), group_prefix=None) #(train:val)

# # define the undersampling method
# undersample = CondensedNearestNeighbour(n_neighbors=1)
# # transform the dataset
# X, y = undersample.fit_resample(X, y)

classes_dir = os.listdir(main_dir + "/train")
n_class = len(classes_dir)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    rotation_range=45,
    horizontal_flip=True,
    fill_mode="nearest"
)
val_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    rotation_range=45,
    horizontal_flip=True,
    fill_mode="nearest"
)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(256,256),
    batch_size=32,
    class_mode="sparse"
)
val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(256,256),
    batch_size=32,
    class_mode="sparse"
)

from tensorflow.keras.models import Sequential
from tensorflow.keras import layers

model = Sequential([
    layers.Conv2D(32, (3,3), strides=(1,1), activation="relu", input_shape=(256,256,3)),
    layers.MaxPooling2D(pool_size=(2,2)),
    layers.Conv2D(64, (3,3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2,2)),
    layers.Conv2D(64, (3,3), activation="relu"),
    layers.MaxPooling2D(pool_size=(2,2)),
    layers.Flatten(),
    layers.Dense(512, activation="relu"),
    layers.Dense(n_class, activation="softmax")
])
model.summary()

import tensorflow as tf
model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer=tf.optimizers.Adam(),
    metrics=["accuracy"]
)

# from google.colab import drive

# drive.mount('/content/gdrive')
# checkpoint = tf.keras.callbacks.ModelCheckpoint(
#     filepath='/content/gdrive/My Drive/ckp/model-{epoch:03d}.ckpt',
#     save_weights_only=True,
#     monitor='val_acc',
#     mode='max',
#     save_best_only=True, 
#     verbose=0)

from keras.callbacks import TensorBoard, Callback

class accuracyStop (Callback) :
    def on_epoch_end(self, epoch, logs={}) :
        if(logs.get("accuracy") > 0.90 and logs.get("val_accuracy") > 0.88) : 
            print("\nTingkat akurasi 90% tercapai!")
            self.model.stop_training = True

callbacks = accuracyStop()

history = model.fit(train_generator, 
                    epochs=50, 
                    validation_data=val_generator, 
                    batch_size=32,
                    steps_per_epoch=100,
                    validation_steps=100,
                    callbacks=[callbacks],
                    verbose=2)

import matplotlib.pyplot as plt

plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.title("Akurasi Model")
plt.ylabel("Akurasi")
plt.xlabel("Epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("Loss Model")
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend(["train", "test"], loc="upper right")
plt.show

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with tf.io.gfile.GFile("model.tflite", "wb") as f :
    f.write(tflite_model)

import numpy as np
from google.colab import files
from keras.preprocessing import image

uploaded = files.upload()

for fn in uploaded.keys():
 
  # predicting images
  path = fn
  img = image.load_img(path, target_size=(150, 150))
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = model.predict(images)
  print(fn)
  print(classes)

keras_model = tf.keras.models.load_model("/content/model.h5")

keras_model.summary()

keras_model.save_weights("/content/model_weight")

os.path.getsize("/content/model_weight.data-00000-of-00001")